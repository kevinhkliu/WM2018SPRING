{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "csv.field_size_limit(2147483647)\n",
    "stemmer = gensim.parsing.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customize_split(sentence, delimiter, start):\n",
    "    sentence = sentence.split(delimiter)\n",
    "    if len(sentence) == 1:\n",
    "        return sentence[0]\n",
    "    else:\n",
    "        return ' '.join(sentence[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    print(\"Predicting...\")\n",
    "    predicted = clf.predict(X_test_tfidf)\n",
    "    with open('result/sklearn_version.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"doc_id\", \"class_id\"])\n",
    "        for i in range(len(predicted)):\n",
    "            writer.writerow([str(i), predicted[i]])\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = []\n",
    "with open(\"data/filters.txt\", \"r\") as f:\n",
    "    filters = f.read().split('\\n')\n",
    "\n",
    "stopwords = []\n",
    "with open(\"data/stop.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "with open('data/groups.csv', 'r', encoding='UTF-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        name = row[1]\n",
    "        categories.append(name)\n",
    "num_category = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "with open('data/doc.csv', 'r', encoding='UTF-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        d = row[1]\n",
    "        test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    ok = []\n",
    "    for document in documents:\n",
    "        document = document.lower()\n",
    "        for p in [(\"\\\\n\\\\n\",1), (\"writes:\",-1), (\"wrote:\",-1)]:\n",
    "            document = customize_split(document, p[0], p[1])\n",
    "        document = document.replace(\"\\\\n\",\" \").replace(\"\\t\",\" \").replace(\"-\",\" \")\n",
    "        document = ' '.join(document.split())\n",
    "        for f in filters:\n",
    "            document = document.replace(f,'')\n",
    "        words = document.split()\n",
    "        clean_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords and len(word)<=11 and len(word)>=3:\n",
    "                word = stemmer.stem_sentence(word)\n",
    "                if word not in clean_words:\n",
    "                    clean_words.append(word)\n",
    "        clean_words = ' '.join(clean_words)\n",
    "        ok.append(clean_words)\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "david davidian subject account anti armenian human right violat azerbaijan summari prelud current event nagorno karabakh sdpa center region studi line ask what go sai matter cant see theyv overturn car theyr kill deposit zaven badasian born emploi sumgait bulk yarn plant resid build apart februari wife went baku shop return around five even ran on rel bu station got talk lot peopl gather far awai near store well first didnt know happen fellow come azerbaijani gui stand home immedi help catch cab safe sat two dai time gang bandit came courtyard neighbor wouldnt let stick piec armatur hand shout someth couldnt understand wasnt voic choru turn toward third _floor break glass throw thing window entrywai pair jean anoth tape record guitar auto part save midnight march hide school famili altogeth known ernest move kirovabad guard want nowher els plead told would attack upstair classroom second floor citi radio announc three telephon number could us summon assist commun anyth import call secretari parti committe answer need evacu wait send spoken person said muslimzad hour heard look outsid get club ax shaft life valu youll left that back door wai point nois mob set direct promis sent instead real soldier posit seen enter knew case stai seven morn sort aid took jam ahead space avail small boi month old di arm singl doctor noth uninjur wound bruis ill gave mouth everyth unabl mother father young coupl search spot child find room put bandag cane limp head broken open terribl sight everyon beaten cry wail think ignor complet true snack bar sausag kopek packag cooki cost sold bottl soft drink rubl cheaper met uncl aram saw tear ey whole friendli work togeth alwai hous cri feel cours april yerevan refer _the tragedi pogrom soviet volum eyewit accounts_ edit samuel forward yelena bonner publish aristid caratza page armenia learn lesson anatolia forgotten box punish inflict cambridg late turkish presid turgut ozal\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0)\n",
    "p = preprocessing(train.data)\n",
    "print(p[0])\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(p)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB(alpha=0.05).fit(X_train_tfidf, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "sure basher pen fan pretti confus lack kind post recent massacr devil actual bit puzzl reliev howev go put end non relief prais man kill wors thought jagr show much better regular season stat also lot fun watch playoff bowman let next coupl game sinc beat pulp jersei anywai see island lose final rule\n",
      "Predicting...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")\n",
    "p = preprocessing(test)\n",
    "print(p[0])\n",
    "X_test_counts = count_vect.transform(p)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
