{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pickle as pk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from __future__ import print_function\n",
    "import codecs\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "tr4w = TextRank4Keyword()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "(kp_sum_posts,kp_posts_time, kp_sum_comments, kp_comments_time, kp_comments_to_post_index) = joblib.load( \"result/kp_all.pkl\" )\n",
    "(yao_sum_posts,yao_posts_time, yao_sum_comments, yao_comments_time, yao_comments_to_post_index) = joblib.load(\"result/yao_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kp posts num: 1526\n",
      "kp comments num: 74705\n",
      "yao posts num: 977\n",
      "yao comments num: 15497\n"
     ]
    }
   ],
   "source": [
    "print('kp posts num: ' + str(len(kp_sum_posts)))\n",
    "print('kp comments num: ' + str(len(kp_sum_comments)))\n",
    "print('yao posts num: ' + str(len(yao_sum_posts)))\n",
    "print('yao comments num: ' + str(len(yao_sum_comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopword len: 2506\n"
     ]
    }
   ],
   "source": [
    "stopword_ch = []\n",
    "with open('data/stopwords.txt', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        stopword_ch.append(line.rstrip())\n",
    "stopword_en = []\n",
    "with open('data/stopwords-en.txt', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        stopword_en.append(line.rstrip())\n",
    "stopword = stopword_ch + stopword_en\n",
    "print('stopword len: ' + str(len(stopword)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(mytext):\n",
    "    jieba_str = ''\n",
    "    tr4w.analyze(text=mytext, lower=True, window=2)\n",
    "    #print(tr4w.words_no_filter)\n",
    "    for words in tr4w.words_no_filter:\n",
    "        word = ' '.join(words)\n",
    "        jieba_str = jieba_str + word + ' '\n",
    "    return jieba_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_jieba(sentence_list):\n",
    "    sentence_seg = []\n",
    "    idx = 0\n",
    "    for sentence in sentence_list:\n",
    "        print(str(idx) + '/' + str(len(sentence_list)))\n",
    "        idx = idx + 1\n",
    "        #comment_message_list.append(jieba_chinese_word_cut(comment))\n",
    "        sentence_seg.append(chinese_word_cut(sentence))\n",
    "    return sentence_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_sum_posts_seg = sentence_jieba(kp_sum_posts)\n",
    "kp_sum_comments_seg = sentence_jieba(kp_sum_comments)\n",
    "yao_sum_posts_seg = sentence_jieba(yao_sum_posts)\n",
    "yao_sum_comments_seg = sentence_jieba(yao_sum_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove English words and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "def sentence_clean(old_seg_list):\n",
    "    seg_list = copy.copy(old_seg_list)\n",
    "    for idx in range(len(seg_list)):\n",
    "        seg_sentence = seg_list[idx]\n",
    "        clean_digts = ''.join(i for i in seg_sentence if not i.isdigit())\n",
    "        clean_eng = [w for w in clean_digts.split(' ') if not re.match(r'[A-Z]+', w, re.I)]\n",
    "        seg_list[idx] = ' '.join(filter(None, clean_eng))\n",
    "    return seg_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_sum_posts_clean_seg = sentence_clean(kp_sum_posts_seg)\n",
    "kp_sum_comments_clean_seg = sentence_clean(kp_sum_comments_seg)\n",
    "yao_sum_posts_clean_seg = sentence_clean(yao_sum_posts_seg)\n",
    "yao_sum_comments_clean_seg = sentence_clean(yao_sum_comments_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Term Weighting with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1526 X 17121 TF-IDF-normalized document-term matrix\n",
      "Created 977 X 13900 TF-IDF-normalized document-term matrix\n",
      "Created 74705 X 50000 TF-IDF-normalized document-term matrix\n",
      "Created 15497 X 21698 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "n_features = 50000\n",
    "tf_vectorizer_kp_posts = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopword\n",
    "                               )\n",
    "tf_vectorizer_yao_posts = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopword\n",
    "                               )\n",
    "\n",
    "tf_vectorizer_kp_comments = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopword\n",
    "                               )\n",
    "tf_vectorizer_yao_comments = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopword\n",
    "                               )\n",
    "tf_kp_posts = tf_vectorizer_kp_posts.fit_transform(kp_sum_posts_clean_seg)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (tf_kp_posts.shape[0], tf_kp_posts.shape[1]) )\n",
    "\n",
    "tf_yao_posts = tf_vectorizer_yao_posts.fit_transform(yao_sum_posts_clean_seg)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (tf_yao_posts.shape[0], tf_yao_posts.shape[1]) )\n",
    "\n",
    "tf_kp_comments = tf_vectorizer_kp_comments.fit_transform(kp_sum_comments_clean_seg)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (tf_kp_comments.shape[0], tf_kp_comments.shape[1]) )\n",
    "\n",
    "tf_yao_comments = tf_vectorizer_yao_comments.fit_transform(yao_sum_comments_clean_seg)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (tf_yao_comments.shape[0], tf_yao_comments.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 17121 distinct terms\n",
      "Vocabulary has 13900 distinct terms\n",
      "Vocabulary has 50000 distinct terms\n",
      "Vocabulary has 21698 distinct terms\n"
     ]
    }
   ],
   "source": [
    "tf_kp_posts_feature_names = tf_vectorizer_kp_posts.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(tf_kp_posts_feature_names))\n",
    "\n",
    "tf_yao_posts_feature_names = tf_vectorizer_yao_posts.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(tf_yao_posts_feature_names))\n",
    "\n",
    "tf_kp_comments_feature_names = tf_vectorizer_kp_comments.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(tf_kp_comments_feature_names))\n",
    "\n",
    "tf_yao_comments_feature_names = tf_vectorizer_yao_comments.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(tf_yao_comments_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf_idf/tf_idf_yao_all.pkl']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump((tf_kp_posts,tf_kp_posts_feature_names, kp_sum_posts_clean_seg, tf_kp_comments,tf_kp_comments_feature_names, kp_sum_comments_clean_seg), \"result/tf_idf_kp_all.pkl\")\n",
    "joblib.dump((tf_yao_posts,tf_yao_posts_feature_names, yao_sum_posts_clean_seg, tf_yao_comments,tf_yao_comments_feature_names, yao_sum_comments_clean_seg), \"result/tf_idf_yao_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
